{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy.cli\n",
    "#spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import spacy\n",
    "\n",
    "# LangChain loaders and processing\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import SpacyTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import SpacyTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"Set DEEPSEEK_API_KEY in your .env!\")\n",
    "\n",
    "client = OpenAI(api_key=API_KEY, base_url=\"https://api.deepseek.com\")\n",
    "MODEL = \"deepseek-chat\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 18 source files/chunks before splitting.\n"
     ]
    }
   ],
   "source": [
    "# --- 1) Recursively load .py / .m files from a root folder ---\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.docstore.document import Document\n",
    "import os\n",
    "\n",
    "ROOT_DIR = \"/Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled\"  # change to your repo root if needed\n",
    "ALLOWED_EXTS = {\".py\", \".m\"}\n",
    "IGNORE_DIRS = {\".git\", \".hg\", \".svn\", \"__pycache__\", \"venv\", \".venv\", \"node_modules\", \"build\", \"dist\", \".mypy_cache\", \".pytest_cache\"}\n",
    "\n",
    "def iter_code_filepaths(root: str):\n",
    "    for dirpath, dirnames, filenames in os.walk(root):\n",
    "        # prune ignored dirs\n",
    "        dirnames[:] = [d for d in dirnames if d not in IGNORE_DIRS]\n",
    "        for fname in filenames:\n",
    "            ext = os.path.splitext(fname)[1].lower()\n",
    "            if ext in ALLOWED_EXTS:\n",
    "                yield os.path.join(dirpath, fname)\n",
    "\n",
    "docs = []\n",
    "for fp in iter_code_filepaths(ROOT_DIR):\n",
    "    try:\n",
    "        # TextLoader handles encodings; set autodetect to avoid crashes\n",
    "        loader = TextLoader(fp, encoding=\"utf-8\", autodetect_encoding=True)\n",
    "        docs.extend(loader.load())\n",
    "    except Exception as e:\n",
    "        print(f\"[skip] {fp}: {e}\")\n",
    "\n",
    "print(f\"Loaded {len(docs)} source files/chunks before splitting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 196 chunks.\n"
     ]
    }
   ],
   "source": [
    "# --- 2) Chunk with code-aware separators (still minimal) ---\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1200,          # good default for code\n",
    "    chunk_overlap=150,\n",
    "    separators=[\n",
    "        \"\\nclass \", \"\\nfunction \", \"\\ndef \",  # Python / MATLAB anchors first\n",
    "        \"\\n##\", \"\\n# \", \"\\n\\n\", \"\\n\", \" \", \"\"\n",
    "    ]\n",
    ")\n",
    "chunks = splitter.split_documents(docs)\n",
    "print(f\"Created {len(chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3) Embed & persist (unchanged, but persist explicitly) ---\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "PERSIST_DIR = \"chroma_store\"  # keep stable for the repo\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=PERSIST_DIR,\n",
    ")\n",
    "vectorstore.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùì Query: describe lyfe cycle of the symbol \"b_HH_g\" and  whether matrix can be passed ofr these like different vec for different household?\n",
      "\n",
      "\n",
      "üìö Sources considered:\n",
      "[1] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/simulate_abm.m\n",
      "[2] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/simulate_abm.m\n",
      "[3] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/abmx.m\n",
      "[4] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/simulate_abm.m\n",
      "[5] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/search_and_matching.m\n",
      "[6] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/abm.m\n",
      "[7] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/simulate_abm.m\n",
      "[8] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/epsilon.m\n",
      "[9] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/abmx.m\n",
      "[10] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/import_abmx.m\n",
      "[11] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/simulate_abm.m\n",
      "[12] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/search_and_matching.m\n",
      "\n",
      "\n",
      "üß† Answer (documentation-driven):\n",
      "Based on the provided context, I will describe the lifecycle of the symbol `b_HH_g` and address whether matrices can be passed for different vectors of households.\n",
      "\n",
      "## Executive Summary\n",
      "`b_HH_g` is a parameter vector that represents household consumption preferences across different sectors (groups) in an Agent-Based Macroeconomic (ABM) model. It is loaded from parameter files, used in search and matching algorithms for consumption allocation, and remains constant throughout simulations. The context suggests it is structured as a vector per sector group (`g`), not as a matrix for different household vectors.\n",
      "\n",
      "## 1. What It Is / Purpose\n",
      "`b_HH_g` is a parameter that governs household consumption preferences across different economic sectors/groups. It represents the budget share or consumption propensity of households toward goods from sector group `g` [7], [12]. In the search and matching process, it helps determine how household consumption demand (`C_d_h`) is allocated across different sectors [12].\n",
      "\n",
      "## 2. Where It Lives (Paths, Line Spans)\n",
      "The symbol `b_HH_g` appears in multiple locations:\n",
      "\n",
      "- **Parameter initialization** in `/simulate_abm.m` [7], [11]:\n",
      "  ```matlab\n",
      "  b_HH_g=parameters.b_HH_g;\n",
      "  ```\n",
      "- **Function parameter** in the `abmx` function definition [9]:\n",
      "  ```matlab\n",
      "  function [...] = abmx(..., b_HH_g, ...)\n",
      "  ```\n",
      "- **Usage** in the `search_and_matching` function [12]:\n",
      "  ```matlab\n",
      "  function [...] = search_and_matching(..., b_HH_g, ...)\n",
      "  ```\n",
      "\n",
      "## 3. API/Contracts: Inputs, Outputs, Side Effects\n",
      "- **Input**: `b_HH_g` is an input parameter to both the main `abmx` function [9] and the `search_and_matching` function [12].\n",
      "- **Output**: It is not directly modified or returned as output from any function.\n",
      "- **Side Effects**: No direct side effects. It influences consumption allocation calculations.\n",
      "- **Data Type**: Based on context, it appears to be a vector of length `G` (number of sector groups) [7], [12].\n",
      "\n",
      "## 4. How It Is Used (Call Sites)\n",
      "`b_HH_g` is used in the consumption matching process within `search_and_matching.m` [12]:\n",
      "\n",
      "```matlab\n",
      "function [Q_d_i,Q_d_m,P_bar_i,DM_i,P_CF_i,I_i,P_bar_h,C_h,P_bar_CF_h,I_h,P_j,C_j,P_l,C_l]=search_and_matching(P_i,Y_i,S_i,S_i_,G_i,P_m,Y_m,a_sg,DM_d_i,b_CF_g,I_d_i,b_HH_g,C_d_h,b_CFH_g,I_d_h,c_G_g,C_d_j,c_E_g,C_d_l)\n",
      "```\n",
      "\n",
      "While the exact usage within the function isn't shown in the context, its presence alongside other budget parameters (`b_CF_g`, `b_CFH_g`) suggests it helps determine how household consumption demand (`C_d_h`) is allocated across different sector groups.\n",
      "\n",
      "## 5. Edge Cases & Risks\n",
      "- **Dimension Mismatch**: If `b_HH_g` length doesn't match the number of groups `G`, it would cause runtime errors.\n",
      "- **Zero Values**: If any element is zero, households might not consume from that sector.\n",
      "- **Normalization**: The vector might need to sum to 1 if it represents budget shares.\n",
      "- **Constant Parameter**: As a loaded parameter, it doesn't adapt during simulation, which might limit behavioral realism.\n",
      "\n",
      "## 6. Related Symbols\n",
      "- `b_CF_g`: Budget parameter for capital formation by firms [7], [12]\n",
      "- `b_CFH_g`: Budget parameter for capital formation by households [7], [12]  \n",
      "- `C_d_h`: Household consumption demand [12]\n",
      "- `G_i`: Group assignment vector for firms/sectors [2]\n",
      "- `a_sg`: Another sector-group parameter used in search and matching [7], [12]\n",
      "\n",
      "## 7. Matrix vs Vector Question Analysis\n",
      "Based on the context, **`b_HH_g` appears to be a vector, not a matrix** for different household vectors:\n",
      "\n",
      "1. **Declaration**: It's loaded as `parameters.b_HH_g` without indication of matrix structure [7], [11]\n",
      "2. **Usage**: In `search_and_matching`, it's passed alongside scalar `G` (number of groups), not `H` (number of households) [12]\n",
      "3. **Dimension Context**: The function handles household consumption `C_d_h` as a matrix across households, but `b_HH_g` operates at the group level [12]\n",
      "4. **Parallel Parameters**: Other `b_*_g` parameters (`b_CF_g`, `b_CFH_g`) are also group-level, suggesting consistent design\n",
      "\n",
      "The consumption preferences are likely modeled at the sector group level (`g`) rather than being differentiated across individual household types or vectors.\n",
      "\n",
      "## 8. Step-by-Step Trace\n",
      "1. **Initialization**: `b_HH_g` is loaded from parameter files (e.g., `2010Q1.mat`) during simulation setup [11]\n",
      "2. **Parameter Passing**: It's passed to the `abmx` function as part of the extensive parameter set [9]\n",
      "3. **Delegation**: Within `abmx`, it's passed to the `search_and_matching` function [12]\n",
      "4. **Consumption Allocation**: Used to determine how aggregate household consumption is allocated across different economic sectors\n",
      "5. **Persistence**: Remains constant throughout the simulation period\n",
      "\n",
      "## 9. Next Investigation Steps\n",
      "To fully understand `b_HH_g`:\n",
      "1. Examine the complete `search_and_matching.m` implementation to see exact usage\n",
      "2. Check parameter estimation/calibration routines to understand how `b_HH_g` values are determined\n",
      "3. Investigate if there are household-type differentiations elsewhere in the model\n",
      "4. Review model documentation for the economic theory behind consumption allocation\n",
      "5. Examine if there are dynamic versions or learning mechanisms for consumption preferences\n",
      "\n",
      "**Not found in the context**: Specific mathematical formulation of how `b_HH_g` influences consumption allocation, and whether household heterogeneity is modeled through other mechanisms.\n",
      "\n",
      "üîé Source trace (for verification):\n",
      "[1] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/simulate_abm.m\n",
      "[2] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/simulate_abm.m\n",
      "[3] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/abmx.m\n",
      "[4] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/simulate_abm.m\n",
      "[5] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/search_and_matching.m\n",
      "[6] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/abm.m\n",
      "[7] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/simulate_abm.m\n",
      "[8] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/epsilon.m\n",
      "[9] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/abmx.m\n",
      "[10] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/import_abmx.m\n",
      "[11] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/simulate_abm.m\n",
      "[12] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/search_and_matching.m\n",
      "\n",
      "\n",
      "Bye!\n"
     ]
    }
   ],
   "source": [
    "# %% -------------------------------------------\n",
    "# 4) Query loop (documentation-driven, source-traced + log full cell output)\n",
    "import re, json, sys, os\n",
    "from datetime import datetime, timezone\n",
    "from io import StringIO\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "LOG_PATH = \"rag_full_session.log\"   # keep appending full cell output here\n",
    "\n",
    "def _grep_symbol(root_dir: str, symbol: str, max_hits: int = 50, ctx_lines: int = 6):\n",
    "    \"\"\"Lightweight code grep: exact word hits with a few context lines.\"\"\"\n",
    "    if not symbol:\n",
    "        return []\n",
    "    is_symbolish = bool(re.fullmatch(r\"[A-Za-z_][A-Za-z0-9_]*\", symbol)) or (\".\" in symbol)\n",
    "    if not is_symbolish and len(symbol) < 3:\n",
    "        return []\n",
    "\n",
    "    word = re.escape(symbol)\n",
    "    pat = re.compile(rf\"\\b{word}\\b\")\n",
    "    hits = []\n",
    "    for dirpath, dirnames, filenames in os.walk(ROOT_DIR):\n",
    "        dirnames[:] = [d for d in dirnames if d not in IGNORE_DIRS and not d.startswith(\".\")]\n",
    "        for fname in filenames:\n",
    "            ext = os.path.splitext(fname)[1].lower()\n",
    "            if ext not in ALLOWED_EXTS:\n",
    "                continue\n",
    "            fp = os.path.join(dirpath, fname)\n",
    "            try:\n",
    "                with open(fp, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                    lines = f.readlines()\n",
    "                for idx, line in enumerate(lines, 1):\n",
    "                    if pat.search(line):\n",
    "                        start = max(1, idx - ctx_lines)\n",
    "                        end = min(len(lines), idx + ctx_lines)\n",
    "                        snippet = \"\".join(lines[start-1:end])\n",
    "                        snippet = re.sub(pat, r\"<<\\g<0>>>\", snippet)\n",
    "                        meta = {\"source\": fp, \"line\": idx, \"span\": f\"{start}-{end}\"}\n",
    "                        hits.append(Document(page_content=snippet, metadata=meta))\n",
    "                        if len(hits) >= max_hits:\n",
    "                            return hits\n",
    "            except Exception:\n",
    "                continue\n",
    "    return hits\n",
    "\n",
    "def _dedupe_docs(docs):\n",
    "    seen = set(); out = []\n",
    "    for d in docs:\n",
    "        key = (d.metadata.get(\"source\",\"\"), d.metadata.get(\"span\",\"\"), d.page_content.strip())\n",
    "        if key not in seen:\n",
    "            seen.add(key); out.append(d)\n",
    "    return out\n",
    "\n",
    "def _source_table(docs):\n",
    "    lines = []\n",
    "    for i, d in enumerate(docs, 1):\n",
    "        src = d.metadata.get(\"source\",\"?\")\n",
    "        span = d.metadata.get(\"span\",\"\")\n",
    "        line = d.metadata.get(\"line\",\"\")\n",
    "        loc = f\":{line}\" if line else \"\"\n",
    "        span_txt = f\" (lines {span})\" if span else \"\"\n",
    "        lines.append(f\"[{i}] {src}{loc}{span_txt}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def ask(query: str, k: int = 12, max_context_chars: int = 120_000):\n",
    "    # -------- Capture all stdout into a buffer --------\n",
    "    old_stdout = sys.stdout\n",
    "    buffer = StringIO()\n",
    "    sys.stdout = buffer\n",
    "    try:\n",
    "        if not query or not query.strip():\n",
    "            print(\"Please enter a non-empty question.\")\n",
    "            return \"\"\n",
    "\n",
    "        print(f\"\\n‚ùì Query: {query}\\n\")\n",
    "\n",
    "        fetch_k = max(k * 3, k + 8)\n",
    "        retriever = vectorstore.as_retriever(\n",
    "            search_type=\"mmr\",\n",
    "            search_kwargs={\"k\": k, \"fetch_k\": fetch_k}\n",
    "        )\n",
    "        emb_docs = retriever.invoke(query) or []\n",
    "        grep_docs = _grep_symbol(ROOT_DIR, query.strip(), max_hits=60, ctx_lines=6)\n",
    "\n",
    "        combined = _dedupe_docs(emb_docs + grep_docs)\n",
    "        if not combined:\n",
    "            print(\"No relevant chunks found.\")\n",
    "            print(\"üß† Answer:\\nNot found in the context.\")\n",
    "            return \"Not found in the context.\"\n",
    "\n",
    "        src_table = _source_table(combined)\n",
    "        raw_context = \"\\n\\n\".join(f\"[{i}] {d.page_content}\" for i, d in enumerate(combined, 1))\n",
    "        if len(raw_context) > max_context_chars:\n",
    "            raw_context = raw_context[:max_context_chars]\n",
    "\n",
    "        print(\"\\nüìö Sources considered:\\n\" + src_table + \"\\n\")\n",
    "\n",
    "        prompt = f\"\"\"You are a senior code assistant that answers PROACTIVELY and THOROUGHLY.\n",
    "RULES:\n",
    "- ONLY use the given context below. If unknown, reply exactly: 'Not found in the context.'\n",
    "- Write a long, well-structured answer.\n",
    "- Heavily ground every claim with inline citations like [1], [2] that refer to the source list.\n",
    "- Prefer quoting small, relevant code lines (short quotes) when clarifying a point, each with a citation.\n",
    "- If the query looks like a symbol (e.g., b_HH), locate DEFINITION(S), ASSIGNMENTS, REFERENCES, and USAGE patterns.\n",
    "- Provide: (1) Executive summary; (2) What it is / purpose; (3) Where it lives (paths, line spans);\n",
    "  (4) API/contracts: inputs, outputs, side effects; (5) How it is used (call sites);\n",
    "  (6) Edge cases & risks; (7) Related symbols; (8) Step-by-step trace to answer the question; (9) Next investigation steps.\n",
    "\n",
    "SOURCES:\n",
    "{src_table}\n",
    "\n",
    "CONTEXT:\n",
    "{raw_context}\n",
    "\n",
    "QUESTION:\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "        try:\n",
    "            resp = client.chat.completions.create(\n",
    "                model=MODEL,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=3000,\n",
    "                temperature=0.1,\n",
    "            )\n",
    "            answer = resp.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            answer = f\"Error calling model: {e}\"\n",
    "\n",
    "        print(\"\\nüß† Answer (documentation-driven):\\n\" + answer)\n",
    "        print(\"\\nüîé Source trace (for verification):\\n\" + src_table + \"\\n\")\n",
    "\n",
    "        return answer\n",
    "    finally:\n",
    "        # -------- Restore stdout and write to file --------\n",
    "        sys.stdout = old_stdout\n",
    "        output = buffer.getvalue()\n",
    "        with open(LOG_PATH, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"\\n--- {datetime.now(timezone.utc).isoformat()} ---\\n\")\n",
    "            f.write(output)\n",
    "            f.write(\"\\n\")\n",
    "        print(output)   # also show in cell as usual\n",
    "\n",
    "\n",
    "# %% -------------------------------------------\n",
    "# Run interactively (multi-question loop; blank/exit to quit)\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        while True:\n",
    "            q = input(\"Ask a question about your codebase (blank or 'exit' to quit): \").strip()\n",
    "            if not q or q.lower() in {\"exit\", \"quit\"}:\n",
    "                print(\"Bye!\")\n",
    "                break\n",
    "            ask(q, k=12, max_context_chars=120_000)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nInterrupted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùì Query: how to incorprate imitation based dynamics in the existing code in a least invasive way?\n",
      "\n",
      "\n",
      "üìö Sources considered:\n",
      "[1] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/search_and_matching_labor.m\n",
      "[2] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/simulate_abm.m\n",
      "[3] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/abm.m\n",
      "[4] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/search_and_matching.m\n",
      "[5] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/abmx.m\n",
      "[6] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/rfvar3.m\n",
      "[7] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/rfvar3.m\n",
      "[8] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/abm.m\n",
      "[9] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/simulate_abm.m\n",
      "[10] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/search_and_matching_credit.m\n",
      "[11] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/simulate_abm.m\n",
      "[12] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/abmx.m\n",
      "\n",
      "\n",
      "üß† Answer (documentation-driven):\n",
      "Not found in the context.\n",
      "\n",
      "Based on the provided context, there is no explicit mention or implementation of \"imitation-based dynamics\" in any of the source files. The codebase appears to focus on:\n",
      "\n",
      "1. **Search and matching algorithms** for labor markets [1, 8] and credit markets [10]\n",
      "2. **Economic simulation** with agent-based modeling approaches [2, 3, 5, 8, 9, 12]\n",
      "3. **Statistical estimation** using VAR models [6, 7]\n",
      "4. **Economic calculations** involving production, consumption, prices, and profits [2, 3, 5, 8]\n",
      "\n",
      "The context contains implementations of:\n",
      "- Labor market matching between workers and firms [1]\n",
      "- Credit market matching between firms and lenders [10]\n",
      "- Price setting mechanisms based on cost factors [12]\n",
      "- Production functions and capacity constraints [8]\n",
      "- Economic variable estimation using predictive models [12]\n",
      "\n",
      "However, none of these implementations reference or incorporate imitation-based dynamics, which would typically involve agents copying behaviors, strategies, or decisions from other agents in the system.\n",
      "\n",
      "üîé Source trace (verify paths/lines):\n",
      "[1] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/search_and_matching_labor.m\n",
      "[2] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/simulate_abm.m\n",
      "[3] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/abm.m\n",
      "[4] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/search_and_matching.m\n",
      "[5] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/abmx.m\n",
      "[6] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/rfvar3.m\n",
      "[7] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/rfvar3.m\n",
      "[8] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/abm.m\n",
      "[9] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/simulate_abm.m\n",
      "[10] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/search_and_matching_credit.m\n",
      "[11] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/simulate_abm.m\n",
      "[12] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/abmx.m\n",
      "\n",
      "Bye!\n"
     ]
    }
   ],
   "source": [
    "# %% -------------------------------------------\n",
    "# 4) Query loop (agentic, documentation-driven)\n",
    "import re\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "def _grep_symbol(root_dir: str, symbol: str, max_hits: int = 60, ctx_lines: int = 6):\n",
    "    \"\"\"Exact-word grep with a few context lines & highlighting.\"\"\"\n",
    "    if not symbol:\n",
    "        return []\n",
    "    word = re.escape(symbol)\n",
    "    pat = re.compile(rf\"\\b{word}\\b\")\n",
    "    hits = []\n",
    "    for dirpath, dirnames, filenames in os.walk(ROOT_DIR):\n",
    "        dirnames[:] = [d for d in dirnames if d not in IGNORE_DIRS and not d.startswith(\".\")]\n",
    "        for fname in filenames:\n",
    "            ext = os.path.splitext(fname)[1].lower()\n",
    "            if ext not in ALLOWED_EXTS:\n",
    "                continue\n",
    "            fp = os.path.join(dirpath, fname)\n",
    "            try:\n",
    "                with open(fp, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                    lines = f.readlines()\n",
    "                for idx, line in enumerate(lines, 1):\n",
    "                    if pat.search(line):\n",
    "                        start = max(1, idx - ctx_lines)\n",
    "                        end = min(len(lines), idx + ctx_lines)\n",
    "                        snippet = \"\".join(lines[start-1:end])\n",
    "                        snippet = pat.sub(lambda m: f\"<<{m.group(0)}>>\", snippet)\n",
    "                        hits.append(Document(page_content=snippet, metadata={\"source\": fp, \"line\": idx, \"span\": f\"{start}-{end}\"}))\n",
    "                        if len(hits) >= max_hits:\n",
    "                            return hits\n",
    "            except Exception:\n",
    "                continue\n",
    "    return hits\n",
    "\n",
    "def _grep_fuzzy(root_dir: str, token: str, max_hits: int = 40, ctx_lines: int = 4):\n",
    "    \"\"\"Fuzzy (substring, case-insensitive) grep for when exact match fails.\"\"\"\n",
    "    if not token or len(token) < 3:\n",
    "        return []\n",
    "    pat = re.compile(re.escape(token), re.IGNORECASE)\n",
    "    hits = []\n",
    "    for dirpath, dirnames, filenames in os.walk(ROOT_DIR):\n",
    "        dirnames[:] = [d for d in dirnames if d not in IGNORE_DIRS and not d.startswith(\".\")]\n",
    "        for fname in filenames:\n",
    "            ext = os.path.splitext(fname)[1].lower()\n",
    "            if ext not in ALLOWED_EXTS:\n",
    "                continue\n",
    "            fp = os.path.join(dirpath, fname)\n",
    "            try:\n",
    "                with open(fp, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                    lines = f.readlines()\n",
    "                for idx, line in enumerate(lines, 1):\n",
    "                    if pat.search(line):\n",
    "                        start = max(1, idx - ctx_lines)\n",
    "                        end = min(len(lines), idx + ctx_lines)\n",
    "                        snippet = \"\".join(lines[start-1:end])\n",
    "                        snippet = pat.sub(lambda m: f\"<<{m.group(0)}>>\", snippet)\n",
    "                        hits.append(Document(page_content=snippet, metadata={\"source\": fp, \"line\": idx, \"span\": f\"{start}-{end}\"}))\n",
    "                        if len(hits) >= max_hits:\n",
    "                            return hits\n",
    "            except Exception:\n",
    "                continue\n",
    "    return hits\n",
    "\n",
    "def _dedupe_docs(docs):\n",
    "    seen = set(); out = []\n",
    "    for d in docs:\n",
    "        key = (d.metadata.get(\"source\",\"\"), d.metadata.get(\"span\",\"\"), d.page_content.strip())\n",
    "        if key not in seen:\n",
    "            seen.add(key); out.append(d)\n",
    "    return out\n",
    "\n",
    "def _source_table(docs):\n",
    "    return \"\\n\".join(\n",
    "        f\"[{i}] {d.metadata.get('source','?')}\" +\n",
    "        (f\":{d.metadata.get('line')}\" if d.metadata.get('line') else \"\") +\n",
    "        (f\" (lines {d.metadata.get('span')})\" if d.metadata.get('span') else \"\")\n",
    "        for i, d in enumerate(docs, 1)\n",
    "    )\n",
    "\n",
    "def _prompt_for_terms(prompt_text: str):\n",
    "    extra = input(prompt_text).strip()\n",
    "    if not extra:\n",
    "        return []\n",
    "    # split on commas/space; keep non-empty\n",
    "    terms = [t for t in re.split(r\"[,\\s]+\", extra) if t]\n",
    "    return terms[:10]\n",
    "\n",
    "def ask(query: str, k: int = 12, max_context_chars: int = 120_000, proactive_rounds: int = 2):\n",
    "    \"\"\"\n",
    "    Agentic, documentation-driven Q&A with clarifying questions and on-demand grep expansion.\n",
    "    - Asks back for extra identifiers/keywords if retrieval is thin.\n",
    "    - Uses exact grep first, then fuzzy grep if still thin.\n",
    "    - Generates long, source-cited answers.\n",
    "    \"\"\"\n",
    "    if not query or not query.strip():\n",
    "        print(\"Please enter a non-empty question.\")\n",
    "        return \"\"\n",
    "\n",
    "    print(f\"\\n‚ùì Query: {query}\\n\")\n",
    "\n",
    "    def retrieve_with(query_str: str, extra_terms: list[str] = None):\n",
    "        # Embedding retriever with MMR\n",
    "        fetch_k = max(k * 3, k + 8)\n",
    "        retriever = vectorstore.as_retriever(\n",
    "            search_type=\"mmr\",\n",
    "            search_kwargs={\"k\": k, \"fetch_k\": fetch_k}\n",
    "        )\n",
    "        emb_docs = retriever.invoke(query_str) or []\n",
    "\n",
    "        # Grep for the main query if it looks like a symbol\n",
    "        grep_docs = _grep_symbol(ROOT_DIR, query_str.strip(), max_hits=60, ctx_lines=6)\n",
    "\n",
    "        # Grep for any extra terms (exact), then (if needed) fuzzy\n",
    "        extra_docs = []\n",
    "        if extra_terms:\n",
    "            for t in extra_terms:\n",
    "                extra_docs.extend(_grep_symbol(ROOT_DIR, t, max_hits=40, ctx_lines=6))\n",
    "\n",
    "        combined = _dedupe_docs(emb_docs + grep_docs + extra_docs)\n",
    "\n",
    "        return combined\n",
    "\n",
    "    # 1) First pass\n",
    "    combined = retrieve_with(query)\n",
    "\n",
    "    # 2) Agentic clarification rounds if thin\n",
    "    round_i = 0\n",
    "    while len(combined) < 4 and round_i < proactive_rounds:\n",
    "        round_i += 1\n",
    "        print(\"‚ö†Ô∏è  Limited evidence found. Help me widen the search.\")\n",
    "        terms = _prompt_for_terms(\"Add identifiers/keywords to grep (comma/space separated), or press Enter to skip: \")\n",
    "        if terms:\n",
    "            combined = retrieve_with(query, extra_terms=terms)\n",
    "\n",
    "        # Still thin? Try fuzzy grep over user-provided terms or split query tokens\n",
    "        if len(combined) < 4:\n",
    "            seed_terms = terms or [t for t in re.split(r\"[,\\s]+\", query) if t]\n",
    "            fuzzy_docs = []\n",
    "            for t in seed_terms[:6]:\n",
    "                fuzzy_docs.extend(_grep_fuzzy(ROOT_DIR, t, max_hits=20, ctx_lines=4))\n",
    "            combined = _dedupe_docs(combined + fuzzy_docs)\n",
    "\n",
    "        if not terms and len(combined) < 4:\n",
    "            # If user didn't add terms and still thin, break to avoid looping\n",
    "            break\n",
    "\n",
    "    if not combined:\n",
    "        print(\"No relevant chunks found.\")\n",
    "        print(\"üß† Answer:\\nNot found in the context.\")\n",
    "        return \"Not found in the context.\"\n",
    "\n",
    "    # Build source pack\n",
    "    src_table = _source_table(combined)\n",
    "    raw_context = \"\\n\\n\".join(f\"[{i}] {d.page_content}\" for i, d in enumerate(combined, 1))\n",
    "    if len(raw_context) > max_context_chars:\n",
    "        raw_context = raw_context[:max_context_chars]\n",
    "\n",
    "    # Show sources considered\n",
    "    print(\"\\nüìö Sources considered:\\n\" + src_table + \"\\n\")\n",
    "\n",
    "    # Structured, documentation-driven prompt\n",
    "    prompt = f\"\"\"You are a senior code assistant that answers PROACTIVELY and THOROUGHLY.\n",
    "RULES:\n",
    "- ONLY use the given context below. If unknown, reply exactly: 'Not found in the context.'\n",
    "- Write a long, well-structured answer with explicit subsections.\n",
    "- Heavily ground every claim with inline citations like [1], [2] that refer to the source list.\n",
    "- Prefer quoting short, relevant code lines with a citation.\n",
    "- If the query looks like a symbol, locate DEFINITIONS, ASSIGNMENTS, REFERENCES, and USAGE patterns.\n",
    "- Provide: (1) Executive summary; (2) What it is / purpose; (3) Where it lives (paths, line spans);\n",
    "  (4) API/contracts: inputs, outputs, side effects; (5) How it is used (call sites);\n",
    "  (6) Edge cases & risks; (7) Related symbols; (8) Step-by-step trace to answer the question; (9) Next steps.\n",
    "\n",
    "SOURCES:\n",
    "{src_table}\n",
    "\n",
    "CONTEXT:\n",
    "{raw_context}\n",
    "\n",
    "QUESTION:\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=3500,\n",
    "            temperature=0.1,\n",
    "        )\n",
    "        answer = resp.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        answer = f\"Error calling model: {e}\"\n",
    "\n",
    "    print(\"\\nüß† Answer (documentation-driven):\\n\" + answer)\n",
    "    print(\"\\nüîé Source trace (verify paths/lines):\\n\" + src_table + \"\\n\")\n",
    "\n",
    "    # Offer to dig deeper\n",
    "    cmd = input(\"Type 'more a,b,c' to grep more terms, or press Enter to continue: \").strip()\n",
    "    if cmd.lower().startswith(\"more\"):\n",
    "        extra = [t for t in re.split(r\"[,\\s]+\", cmd[4:]) if t]\n",
    "        if extra:\n",
    "            # Re-run with extra terms\n",
    "            print(f\"\\nüîÅ Deepening search with: {extra}\\n\")\n",
    "            return ask(query, k=k, max_context_chars=max_context_chars, proactive_rounds=0)\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n",
    "# %% -------------------------------------------\n",
    "# Run interactively (multi-question loop; commands supported)\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        while True:\n",
    "            q = input(\n",
    "                \"Ask about your codebase (blank/'exit' to quit).\\n\"\n",
    "                \"Tip: prefix with 'more a,b,c' later to deepen grep.\\n> \"\n",
    "            ).strip()\n",
    "            if not q or q.lower() in {\"exit\", \"quit\"}:\n",
    "                print(\"Bye!\")\n",
    "                break\n",
    "            # Shortcut to inject grep terms explicitly:\n",
    "            if q.lower().startswith(\"more \"):\n",
    "                print(\"Provide an actual question first, then use 'more a,b,c' when prompted.\\n\")\n",
    "                continue\n",
    "            ask(q, k=12, max_context_chars=120_000, proactive_rounds=2)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nInterrupted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "‚ùì Query: how to modify the code base with minial changes keeoing the esmenatics intact to pass a matrix for b_HH_g\n",
    "\n",
    "\n",
    "üìö Sources considered:\n",
    "[1] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/simulate_abm.m\n",
    "[2] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/abmx.m\n",
    "[3] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/search_and_matching.m\n",
    "[4] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/simulate_abm.m\n",
    "[5] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/epsilon.m\n",
    "[6] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/simulate_abm.m\n",
    "[7] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/rfvar3.m\n",
    "[8] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/rfvar3.m\n",
    "[9] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/abm.m\n",
    "[10] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/simulate_abm.m\n",
    "[11] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/import_abmx.m\n",
    "[12] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/search_and_matching.m\n",
    "\n",
    "\n",
    "üß† Answer (documentation-driven):\n",
    "Based on the provided context, I will analyze how to modify the code base to pass a matrix for `b_HH_g` while keeping semantics intact.\n",
    "\n",
    "## Executive Summary\n",
    "To modify the code base to accept a matrix for `b_HH_g` instead of a vector, minimal changes are required primarily in the `search_and_matching` function and its call sites. The key modifications involve: (1) Updating function signatures to handle matrix input, (2) Modifying the matching algorithm to process sector-specific household preferences, and (3) Ensuring proper dimension handling throughout the consumption matching process. The changes maintain the existing semantics while enabling more granular household consumption preferences across different sectors.\n",
    "\n",
    "## What it is / Purpose\n",
    "The `b_HH_g` parameter represents household consumption preferences across different sectors/groups. Currently implemented as a vector `b_HH_g(1,G)` where G is the number of sectors/groups, it would be modified to a matrix `b_HH_g(H,G)` where H is the number of households and G is the number of sectors. This allows each household to have unique consumption preferences across different economic sectors, enabling more realistic heterogeneous consumer behavior [3][12].\n",
    "\n",
    "## Where it lives (paths, line spans)\n",
    "- **Definition/Initialization**: `b_HH_g` is defined as a parameter in the main simulation file [4]\n",
    "- **Usage**: Primarily used in the `search_and_matching` function [3] and consumption matching algorithm [12]\n",
    "- **Current implementation**: Vector of size `(1,G)` where G is number of sectors/groups\n",
    "\n",
    "## API/Contracts: Inputs, Outputs, Side Effects\n",
    "**Current API:**\n",
    "- Input: `b_HH_g` as vector `(1,G)`\n",
    "- Output: Aggregate consumption matching across sectors\n",
    "\n",
    "**Modified API:**\n",
    "- Input: `b_HH_g` as matrix `(H,G)` where H = number of households, G = number of sectors\n",
    "- Output: Household-specific consumption patterns with maintained aggregate semantics\n",
    "- Side effects: Requires updates to dimension handling in matching algorithms\n",
    "\n",
    "## How it is used (call sites)\n",
    "The `b_HH_g` parameter is used in the consumption matching process within `search_and_matching`:\n",
    "\n",
    "```matlab\n",
    "function [Q_d_i,Q_d_m,P_bar_i,DM_i,P_CF_i,I_i,P_bar_h,C_h,P_bar_CF_h,I_h,P_j,C_j,P_l,C_l]=search_and_matching(P_i,\n",
    "\n",
    "Y_i,S_i,S_i_,G_i,P_m,Y_m,a_sg,DM_d_i,b_CF_g,I_d_i,b_HH_g,C_d_h,b_CFH_g,I_d_h,c_G_g,C_d_j,c_E_g,C_d_l)\n",
    "```\n",
    "\n",
    "And in the consumption matching algorithm [12]:\n",
    "```matlab\n",
    "H_g=find(C_d_hg>0);\n",
    "F_g=find(G_f==g);\n",
    "```\n",
    "\n",
    "## Edge Cases & Risks\n",
    "1. **Dimension mismatches**: Ensuring `size(b_HH_g,1) == H` and `size(b_HH_g,2) == G`\n",
    "2. **Zero preferences**: Handling households with zero preference for certain sectors\n",
    "3. **Normalization issues**: Maintaining that `sum(b_HH_g(h,:)) ‚âà 1` for each household\n",
    "4. **Backward compatibility**: Ensuring existing vector inputs still work with proper validation\n",
    "\n",
    "## Related Symbols\n",
    "- `C_d_h`: Household consumption demand [3]\n",
    "- `G_f`: Sector grouping of firms [3][12]\n",
    "- `H_g`: Household groups by sector preference [12]\n",
    "- `a_sg`: Sectoral productivity parameters [3][4]\n",
    "- `b_CF_g`, `b_CFH_g`: Other preference parameters [3][4]\n",
    "\n",
    "## Step-by-Step Trace to Modify the Code\n",
    "\n",
    "### Step 1: Update Function Signatures\n",
    "Modify `search_and_matching` to accept matrix `b_HH_g`:\n",
    "\n",
    "```matlab\n",
    "function [Q_d_i,Q_d_m,P_bar_i,DM_i,P_CF_i,I_i,P_bar_h,C_h,P_bar_CF_h,I_h,P_j,C_j,P_l,C_l]=search_and_matching(P_i,\n",
    "\n",
    "Y_i,S_i,S_i_,G_i,P_m,Y_m,a_sg,DM_d_i,b_CF_g,I_d_i,b_HH_g,C_d_h,b_CFH_g,I_d_h,c_G_g,C_d_j,c_E_g,C_d_l)\n",
    "\n",
    "% Add dimension validation\n",
    "H = size(C_d_h,2);\n",
    "if size(b_HH_g,1) == 1 && size(b_HH_g,2) > 1\n",
    "    % Backward compatibility: expand vector to matrix\n",
    "    b_HH_g = repmat(b_HH_g, H, 1);\n",
    "elseif size(b_HH_g,1) ~= H || size(b_HH_g,2) ~= G\n",
    "    error('b_HH_g must be of size (H,G) or (1,G)');\n",
    "end\n",
    "```\n",
    "\n",
    "### Step 2: Modify Consumption Matching Algorithm\n",
    "Update the matching logic in [12] to handle household-specific preferences:\n",
    "\n",
    "```matlab\n",
    "% Replace sector-level matching with household-sector level matching\n",
    "for g=1:G\n",
    "    % Get household demands for this sector weighted by their preferences\n",
    "    C_d_hg = b_HH_g(:,g)' .* C_d_h;  % Element-wise multiplication\n",
    "    H_g = find(C_d_hg > 0);\n",
    "    \n",
    "    if ~isempty(H_g)\n",
    "        C_d_hg_ = C_d_hg;\n",
    "        F_g = find(G_f == g);\n",
    "        F_g(S_fg_(F_g) <= 0 | S_f(F_g) <= 0) = [];\n",
    "        \n",
    "        while ~isempty(H_g) && ~isempty(F_g)\n",
    "            % Existing matching logic but now with household-specific demands\n",
    "            pr_price_f = max(0, exp(-2*P_f(F_g)) ./ sum(exp(-2*P_f(F_g))));\n",
    "            pr_size_f = S_f(F_g) / sum(S_f(F_g));\n",
    "            pr_cum_f = [0, cumsum(pr_price_f + pr_size_f) / sum(pr_price_f + pr_size_f)];\n",
    "            \n",
    "            H_g = H_g(randperm(length(H_g)));\n",
    "            for j=1:length(H_g)\n",
    "                h = H_g(j);\n",
    "                e = randf(pr_cum_f);\n",
    "                f = F_g(e);\n",
    "                \n",
    "                if S_fg_(f) > C_d_hg_(h) / P_f(f)\n",
    "                    S_fg(f) = S_fg(f) - C_d_hg_(h) / P_f(f);\n",
    "                    S_fg_(f) = S_fg_(f) - C_d_hg_(h) / P_f(f);\n",
    "                    C_d_hg_(h) = 0;\n",
    "                else\n",
    "                    C_d_hg_(h) = C_d_hg_(h) - S_fg_(f) * P_f(f);\n",
    "                    S_fg(f) = S_fg(f) - S_fg_(f);\n",
    "                    S_fg_(f) = 0;\n",
    "                    F_g(e) = [];\n",
    "                    if isempty(F_g)\n",
    "                        break\n",
    "                    end\n",
    "                    pr_price_f = max(0, exp(-2*P_f(F_g)) ./ sum(exp(-2*P_f(F_g))));\n",
    "                end\n",
    "            end\n",
    "            H_g = find(C_d_hg_ > 0);\n",
    "        end\n",
    "    end\n",
    "end\n",
    "```\n",
    "\n",
    "### Step 3: Update Parameter Loading\n",
    "Ensure proper loading of the matrix parameter [4][11]:\n",
    "\n",
    "```matlab\n",
    "% In parameter loading section\n",
    "if isvector(b_HH_g) && length(b_HH_g) == G\n",
    "    % Convert vector to matrix for backward compatibility\n",
    "    b_HH_g = repmat(b_HH_g(:)', H, 1);\n",
    "end\n",
    "```\n",
    "\n",
    "### Step 4: Maintain Aggregate Statistics\n",
    "Ensure macroeconomic aggregates still compute correctly [9]:\n",
    "\n",
    "```matlab\n",
    "% These should remain unchanged as they aggregate across households\n",
    "nominal_household_consumption(t) = (1+tau_VAT)*sum(C_h);\n",
    "real_household_consumption(t) = (1+tau_VAT)*sum(C_h)/P_bar_h;\n",
    "```\n",
    "\n",
    "## Next Steps\n",
    "1. **Test backward compatibility**: Verify that vector inputs still work correctly\n",
    "2. **Add validation**: Implement comprehensive dimension checking throughout the codebase\n",
    "3. **Update documentation**: Document the new matrix format for `b_HH_g`\n",
    "4. **Performance testing**: Ensure the modified matching algorithm maintains reasonable performance with larger matrix inputs\n",
    "5. **Add examples**: Provide sample data showing the matrix format for `b_HH_g`\n",
    "\n",
    "The modifications are minimal and focused, maintaining the existing economic semantics while enabling more granular household consumption behavior representation. The key insight is that the matching algorithm already operates at the household-sector level, so the changes primarily involve proper dimension handling and preference weighting.\n",
    "\n",
    "üîé Source trace (verify paths/lines):\n",
    "[1] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/simulate_abm.m\n",
    "[2] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/abmx.m\n",
    "[3] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/search_and_matching.m\n",
    "[4] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/simulate_abm.m\n",
    "[5] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/epsilon.m\n",
    "[6] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/simulate_abm.m\n",
    "[7] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/rfvar3.m\n",
    "[8] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/rfvar3.m\n",
    "[9] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/abm.m\n",
    "[10] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/simulate_abm.m\n",
    "[11] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/import_abmx.m\n",
    "[12] /Users/mimuw2022/Documents/GitHub/deepseek_rag/data_and_programs/model_scaled/search_and_matching.m\n",
    "\n",
    "Bye!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
